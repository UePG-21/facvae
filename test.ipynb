{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from facvae import FactorVAE\n",
    "from facvae.data import shift_ret, get_dataloaders\n",
    "from facvae.pipeline import test_model, train_model\n",
    "from facvae.backtesting import Backtester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "dir_main = os.path.abspath(\"..\") + \"/\"\n",
    "dir_code = dir_main + \"code/\"\n",
    "dir_data = dir_main + \"data/\"\n",
    "dir_config = dir_main + \"config/\"\n",
    "dir_result = dir_main + \"result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "E = 25\n",
    "B = 8\n",
    "N = 74\n",
    "T = 20\n",
    "C = 28\n",
    "H = 10\n",
    "M = 32\n",
    "K = 8\n",
    "h_prior_size = 16\n",
    "h_alpha_size = 16\n",
    "h_prior_size = 16\n",
    "partition = [0.7, 0.2, 0.1]\n",
    "lr = 0.001\n",
    "lmd = 0.0\n",
    "max_grad = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def cat(x: float) -> float:\n",
    "    if x > 0.01:\n",
    "        return 1.0\n",
    "    elif x < -0.01:\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pm1m</th>\n",
       "      <th>annvol12m</th>\n",
       "      <th>annvol1m</th>\n",
       "      <th>pratio15to36w</th>\n",
       "      <th>hl52w</th>\n",
       "      <th>90dcv</th>\n",
       "      <th>50to200prcratio</th>\n",
       "      <th>prcto52wh</th>\n",
       "      <th>prcto260dl</th>\n",
       "      <th>maxretpayoff</th>\n",
       "      <th>...</th>\n",
       "      <th>5dmoneyflowvol</th>\n",
       "      <th>pm5d</th>\n",
       "      <th>logunadjprice</th>\n",
       "      <th>pm-style</th>\n",
       "      <th>vol-style</th>\n",
       "      <th>adjsto_6m</th>\n",
       "      <th>chg1yturnover</th>\n",
       "      <th>amihud</th>\n",
       "      <th>chg1yamihud</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>fsi_sid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2015-01-02</th>\n",
       "      <th>2363</th>\n",
       "      <td>-0.853006</td>\n",
       "      <td>0.657372</td>\n",
       "      <td>0.230831</td>\n",
       "      <td>-0.010812</td>\n",
       "      <td>0.142087</td>\n",
       "      <td>0.099756</td>\n",
       "      <td>-0.473607</td>\n",
       "      <td>-0.440779</td>\n",
       "      <td>-1.025238</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.123729</td>\n",
       "      <td>-1.113864</td>\n",
       "      <td>0.251149</td>\n",
       "      <td>-0.664831</td>\n",
       "      <td>-0.701290</td>\n",
       "      <td>0.907120</td>\n",
       "      <td>-0.788316</td>\n",
       "      <td>-0.377425</td>\n",
       "      <td>1.344542</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>-0.361725</td>\n",
       "      <td>-0.771023</td>\n",
       "      <td>-0.440008</td>\n",
       "      <td>0.241114</td>\n",
       "      <td>-0.745611</td>\n",
       "      <td>-0.166680</td>\n",
       "      <td>0.190226</td>\n",
       "      <td>0.839314</td>\n",
       "      <td>0.132159</td>\n",
       "      <td>-0.417013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085790</td>\n",
       "      <td>0.109447</td>\n",
       "      <td>0.168957</td>\n",
       "      <td>-0.461707</td>\n",
       "      <td>0.280099</td>\n",
       "      <td>-0.411901</td>\n",
       "      <td>-0.008376</td>\n",
       "      <td>-0.503481</td>\n",
       "      <td>-0.082343</td>\n",
       "      <td>-0.003196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>-1.183526</td>\n",
       "      <td>-0.702803</td>\n",
       "      <td>-0.648000</td>\n",
       "      <td>-0.989553</td>\n",
       "      <td>0.397073</td>\n",
       "      <td>-0.856260</td>\n",
       "      <td>-0.934151</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.614879</td>\n",
       "      <td>-0.758530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.575051</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.545824</td>\n",
       "      <td>-0.647155</td>\n",
       "      <td>0.798158</td>\n",
       "      <td>-1.705545</td>\n",
       "      <td>1.537420</td>\n",
       "      <td>0.148732</td>\n",
       "      <td>-1.154659</td>\n",
       "      <td>0.004657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>-1.080143</td>\n",
       "      <td>-0.760415</td>\n",
       "      <td>-0.598295</td>\n",
       "      <td>-1.008328</td>\n",
       "      <td>0.309003</td>\n",
       "      <td>-0.877045</td>\n",
       "      <td>-0.946264</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>-0.628505</td>\n",
       "      <td>-0.824582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164114</td>\n",
       "      <td>-0.082312</td>\n",
       "      <td>0.552549</td>\n",
       "      <td>-0.515808</td>\n",
       "      <td>0.832735</td>\n",
       "      <td>-0.236236</td>\n",
       "      <td>1.994063</td>\n",
       "      <td>-0.655826</td>\n",
       "      <td>-0.530390</td>\n",
       "      <td>-0.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>0.348187</td>\n",
       "      <td>-0.056848</td>\n",
       "      <td>0.699870</td>\n",
       "      <td>-1.084309</td>\n",
       "      <td>1.450180</td>\n",
       "      <td>-0.671718</td>\n",
       "      <td>-0.942724</td>\n",
       "      <td>-0.414149</td>\n",
       "      <td>-1.052401</td>\n",
       "      <td>0.358239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.911981</td>\n",
       "      <td>-0.857756</td>\n",
       "      <td>-0.428483</td>\n",
       "      <td>0.624662</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>-0.130606</td>\n",
       "      <td>1.068354</td>\n",
       "      <td>-0.383378</td>\n",
       "      <td>0.414484</td>\n",
       "      <td>-0.012759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-12-29</th>\n",
       "      <th>5013</th>\n",
       "      <td>-0.643085</td>\n",
       "      <td>1.108016</td>\n",
       "      <td>-0.228310</td>\n",
       "      <td>-0.040278</td>\n",
       "      <td>1.277100</td>\n",
       "      <td>-0.003151</td>\n",
       "      <td>-0.676602</td>\n",
       "      <td>-1.341055</td>\n",
       "      <td>-0.740149</td>\n",
       "      <td>0.115606</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.066939</td>\n",
       "      <td>-0.747724</td>\n",
       "      <td>1.294049</td>\n",
       "      <td>-0.414217</td>\n",
       "      <td>0.189188</td>\n",
       "      <td>0.491051</td>\n",
       "      <td>0.147984</td>\n",
       "      <td>-0.262386</td>\n",
       "      <td>-0.789474</td>\n",
       "      <td>0.006068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>0.089852</td>\n",
       "      <td>-0.946019</td>\n",
       "      <td>-0.919581</td>\n",
       "      <td>0.887375</td>\n",
       "      <td>-0.384340</td>\n",
       "      <td>-0.796665</td>\n",
       "      <td>0.708824</td>\n",
       "      <td>0.771973</td>\n",
       "      <td>-0.093881</td>\n",
       "      <td>-0.317973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603056</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>-0.804720</td>\n",
       "      <td>1.311300</td>\n",
       "      <td>-0.678085</td>\n",
       "      <td>-0.635803</td>\n",
       "      <td>-0.273802</td>\n",
       "      <td>-0.530095</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>0.612618</td>\n",
       "      <td>-0.540199</td>\n",
       "      <td>-0.428574</td>\n",
       "      <td>0.879007</td>\n",
       "      <td>-0.395288</td>\n",
       "      <td>-0.420449</td>\n",
       "      <td>0.956789</td>\n",
       "      <td>0.704532</td>\n",
       "      <td>1.500790</td>\n",
       "      <td>-0.508548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230054</td>\n",
       "      <td>0.223294</td>\n",
       "      <td>0.654865</td>\n",
       "      <td>-0.294792</td>\n",
       "      <td>0.886227</td>\n",
       "      <td>-0.689597</td>\n",
       "      <td>-0.331745</td>\n",
       "      <td>-0.216579</td>\n",
       "      <td>-0.348523</td>\n",
       "      <td>0.085954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>-1.952275</td>\n",
       "      <td>1.910260</td>\n",
       "      <td>2.174899</td>\n",
       "      <td>-2.038032</td>\n",
       "      <td>2.133222</td>\n",
       "      <td>2.025190</td>\n",
       "      <td>-1.913336</td>\n",
       "      <td>-1.892773</td>\n",
       "      <td>-0.989403</td>\n",
       "      <td>1.933792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.875464</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>-2.001278</td>\n",
       "      <td>0.277436</td>\n",
       "      <td>-2.028070</td>\n",
       "      <td>0.884920</td>\n",
       "      <td>-1.491837</td>\n",
       "      <td>2.222999</td>\n",
       "      <td>2.167032</td>\n",
       "      <td>0.008015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>-2.236329</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>1.394505</td>\n",
       "      <td>-0.890556</td>\n",
       "      <td>0.636323</td>\n",
       "      <td>0.134988</td>\n",
       "      <td>-0.841576</td>\n",
       "      <td>-0.951310</td>\n",
       "      <td>-1.154222</td>\n",
       "      <td>-0.372165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.646768</td>\n",
       "      <td>-0.014655</td>\n",
       "      <td>-0.486649</td>\n",
       "      <td>-0.650150</td>\n",
       "      <td>-0.775535</td>\n",
       "      <td>-0.147960</td>\n",
       "      <td>1.242222</td>\n",
       "      <td>0.891175</td>\n",
       "      <td>0.126192</td>\n",
       "      <td>-0.012779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148888 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        pm1m  annvol12m  annvol1m  pratio15to36w     hl52w  \\\n",
       "date       fsi_sid                                                           \n",
       "2015-01-02 2363    -0.853006   0.657372  0.230831      -0.010812  0.142087   \n",
       "           2407    -0.361725  -0.771023 -0.440008       0.241114 -0.745611   \n",
       "           2578    -1.183526  -0.702803 -0.648000      -0.989553  0.397073   \n",
       "           2579    -1.080143  -0.760415 -0.598295      -1.008328  0.309003   \n",
       "           2587     0.348187  -0.056848  0.699870      -1.084309  1.450180   \n",
       "...                      ...        ...       ...            ...       ...   \n",
       "2022-12-29 5013    -0.643085   1.108016 -0.228310      -0.040278  1.277100   \n",
       "           5020     0.089852  -0.946019 -0.919581       0.887375 -0.384340   \n",
       "           5110     0.612618  -0.540199 -0.428574       0.879007 -0.395288   \n",
       "           5117    -1.952275   1.910260  2.174899      -2.038032  2.133222   \n",
       "           5721    -2.236329   0.002080  1.394505      -0.890556  0.636323   \n",
       "\n",
       "                       90dcv  50to200prcratio  prcto52wh  prcto260dl  \\\n",
       "date       fsi_sid                                                     \n",
       "2015-01-02 2363     0.099756        -0.473607  -0.440779   -1.025238   \n",
       "           2407    -0.166680         0.190226   0.839314    0.132159   \n",
       "           2578    -0.856260        -0.934151   0.032715   -0.614879   \n",
       "           2579    -0.877045        -0.946264   0.145282   -0.628505   \n",
       "           2587    -0.671718        -0.942724  -0.414149   -1.052401   \n",
       "...                      ...              ...        ...         ...   \n",
       "2022-12-29 5013    -0.003151        -0.676602  -1.341055   -0.740149   \n",
       "           5020    -0.796665         0.708824   0.771973   -0.093881   \n",
       "           5110    -0.420449         0.956789   0.704532    1.500790   \n",
       "           5117     2.025190        -1.913336  -1.892773   -0.989403   \n",
       "           5721     0.134988        -0.841576  -0.951310   -1.154222   \n",
       "\n",
       "                    maxretpayoff  ...  5dmoneyflowvol      pm5d  \\\n",
       "date       fsi_sid                ...                             \n",
       "2015-01-02 2363        -0.002566  ...       -1.123729 -1.113864   \n",
       "           2407        -0.417013  ...        0.085790  0.109447   \n",
       "           2578        -0.758530  ...       -0.575051 -0.008930   \n",
       "           2579        -0.824582  ...        0.164114 -0.082312   \n",
       "           2587         0.358239  ...       -0.911981 -0.857756   \n",
       "...                          ...  ...             ...       ...   \n",
       "2022-12-29 5013         0.115606  ...       -2.066939 -0.747724   \n",
       "           5020        -0.317973  ...        0.603056  0.048445   \n",
       "           5110        -0.508548  ...       -0.230054  0.223294   \n",
       "           5117         1.933792  ...       -1.875464  0.020398   \n",
       "           5721        -0.372165  ...       -0.646768 -0.014655   \n",
       "\n",
       "                    logunadjprice  pm-style  vol-style  adjsto_6m  \\\n",
       "date       fsi_sid                                                  \n",
       "2015-01-02 2363          0.251149 -0.664831  -0.701290   0.907120   \n",
       "           2407          0.168957 -0.461707   0.280099  -0.411901   \n",
       "           2578          0.545824 -0.647155   0.798158  -1.705545   \n",
       "           2579          0.552549 -0.515808   0.832735  -0.236236   \n",
       "           2587         -0.428483  0.624662   0.023684  -0.130606   \n",
       "...                           ...       ...        ...        ...   \n",
       "2022-12-29 5013          1.294049 -0.414217   0.189188   0.491051   \n",
       "           5020          1.143512 -0.804720   1.311300  -0.678085   \n",
       "           5110          0.654865 -0.294792   0.886227  -0.689597   \n",
       "           5117         -2.001278  0.277436  -2.028070   0.884920   \n",
       "           5721         -0.486649 -0.650150  -0.775535  -0.147960   \n",
       "\n",
       "                    chg1yturnover    amihud  chg1yamihud       ret  \n",
       "date       fsi_sid                                                  \n",
       "2015-01-02 2363         -0.788316 -0.377425     1.344542  0.000385  \n",
       "           2407         -0.008376 -0.503481    -0.082343 -0.003196  \n",
       "           2578          1.537420  0.148732    -1.154659  0.004657  \n",
       "           2579          1.994063 -0.655826    -0.530390 -0.007385  \n",
       "           2587          1.068354 -0.383378     0.414484 -0.012759  \n",
       "...                           ...       ...          ...       ...  \n",
       "2022-12-29 5013          0.147984 -0.262386    -0.789474  0.006068  \n",
       "           5020         -0.635803 -0.273802    -0.530095  0.013300  \n",
       "           5110         -0.331745 -0.216579    -0.348523  0.085954  \n",
       "           5117         -1.491837  2.222999     2.167032  0.008015  \n",
       "           5721          1.242222  0.891175     0.126192 -0.012779  \n",
       "\n",
       "[148888 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(dir_data + \"df_l1_comb.pickle\")\n",
    "df.sort_index(level=(0, 1), inplace=True)\n",
    "df = shift_ret(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"ret\"] = df[\"ret\"].apply(cat)\n",
    "# print(df[\"ret\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_valid, dl_test = get_dataloaders(df, T, B, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Epoch 0 ================\n",
      "tensor([-0.1548, -0.1433, -0.1573, -0.1659, -0.1189, -0.1312, -0.1349, -0.1554,\n",
      "        -0.1522, -0.1454, -0.1343, -0.1332, -0.1389, -0.1485, -0.1256, -0.1303,\n",
      "        -0.1209, -0.1367, -0.1278, -0.1307, -0.1223, -0.1420, -0.1497, -0.1211,\n",
      "        -0.1253, -0.1247, -0.1488, -0.1515, -0.1441, -0.1276, -0.1307, -0.1397,\n",
      "        -0.1480, -0.1276, -0.1350, -0.1183, -0.1327, -0.1244, -0.1275, -0.1215,\n",
      "        -0.1418, -0.1511, -0.1226, -0.1402, -0.1279, -0.1521, -0.1553, -0.1476,\n",
      "        -0.1313, -0.1332, -0.1411, -0.1394, -0.1274, -0.1348, -0.1180, -0.1298,\n",
      "        -0.1243, -0.1272, -0.1213, -0.1425, -0.1508, -0.1222, -0.1401, -0.1615,\n",
      "        -0.1346, -0.1329, -0.1109, -0.1128, -0.1377, -0.1694, -0.1696, -0.1882,\n",
      "        -0.1428, -0.1177], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "batch: 0, loss: 37.067962646484375\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (8, 74)) of distribution MultivariateNormal(loc: torch.Size([8, 74]), covariance_matrix: torch.Size([8, 74, 74])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan]], device='cuda:0', grad_fn=<ExpandBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Others\\Programming\\py_vscode\\modules\\machine_learning\\facvae\\test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Others/Programming/py_vscode/modules/machine_learning/facvae/test.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Others/Programming/py_vscode/modules/machine_learning/facvae/test.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Others/Programming/py_vscode/modules/machine_learning/facvae/test.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_model(fv, dl_train, lr, E, lmd\u001b[39m=\u001b[39;49mlmd, max_grad\u001b[39m=\u001b[39;49mmax_grad)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Others/Programming/py_vscode/modules/machine_learning/facvae/test.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Others/Programming/py_vscode/modules/machine_learning/facvae/test.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m test_model(fv, dl_test)\n",
      "File \u001b[1;32me:\\Others\\Programming\\py_vscode\\modules\\machine_learning\\facvae\\facvae\\pipeline.py:127\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, learning_rate, epochs, lmd, max_grad, opt_family, verbose_freq)\u001b[0m\n\u001b[0;32m    125\u001b[0m out: \u001b[39mtuple\u001b[39m[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m model(x, y)\n\u001b[0;32m    126\u001b[0m \u001b[39mprint\u001b[39m(out[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 127\u001b[0m loss: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m loss_func_vae(y, \u001b[39m*\u001b[39;49mout, lmd)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    128\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    130\u001b[0m \u001b[39m# total_norm = 0.0\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m# for p in model.parameters():\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m#     param_norm = p.grad.data.norm(2)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \n\u001b[0;32m    138\u001b[0m \u001b[39m# clip gradient\u001b[39;00m\n",
      "File \u001b[1;32me:\\Others\\Programming\\py_vscode\\modules\\machine_learning\\facvae\\facvae\\pipeline.py:46\u001b[0m, in \u001b[0;36mloss_func_vae\u001b[1;34m(y, mu_y, Sigma_y, mu_post, sigma_post, mu_prior, sigma_prior, lmd)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_func_vae\u001b[39m(\n\u001b[0;32m     11\u001b[0m     y: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m     12\u001b[0m     mu_y: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     lmd: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m,\n\u001b[0;32m     19\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"Loss function of FactorVAE\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m        Loss values, B, denoted as `loss`\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     dist_y \u001b[39m=\u001b[39m MultivariateNormal(mu_y, Sigma_y)\n\u001b[0;32m     47\u001b[0m     ll \u001b[39m=\u001b[39m dist_y\u001b[39m.\u001b[39mlog_prob(y)\n\u001b[0;32m     48\u001b[0m     \u001b[39m# ll = -((y - mu_y)**2).sum(-1)\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:150\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m=\u001b[39m loc\u001b[39m.\u001b[39mexpand(batch_shape \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[0;32m    149\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 150\u001b[0m \u001b[39msuper\u001b[39;49m(MultivariateNormal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, event_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m scale_tril \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\torch\\distributions\\distribution.py:56\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m     55\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m---> 56\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m             )\n\u001b[0;32m     63\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (8, 74)) of distribution MultivariateNormal(loc: torch.Size([8, 74]), covariance_matrix: torch.Size([8, 74, 74])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan]], device='cuda:0', grad_fn=<ExpandBackward0>)"
     ]
    }
   ],
   "source": [
    "# model\n",
    "fv = FactorVAE(C, H, M, K, h_prior_size, h_alpha_size, h_prior_size).to(\"cuda\")\n",
    "\n",
    "# TODO: constrain the output interval\n",
    "# data\n",
    "def cat(x: float) -> float:\n",
    "    if x > 0.01:\n",
    "        return 1.0\n",
    "    elif x < -0.01:\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "    \n",
    "# train\n",
    "train_model(fv, dl_train, lr, E, lmd=lmd, max_grad=max_grad)\n",
    "\n",
    "# test\n",
    "loss = test_model(fv, dl_test)\n",
    "print(\"out-of-sample loss:\", loss)\n",
    "\n",
    "# predict\n",
    "x, y = next(iter(dl_test))\n",
    "mu_y, Sigma_y = fv.predict(x)\n",
    "\n",
    "# backtest\n",
    "len_test = next(iter(dl_test))[1].shape[0]\n",
    "idx = pd.IndexSlice[df.index.get_level_values(0).unique()[-len_test:], :]\n",
    "df = df.loc[idx]\n",
    "\n",
    "df[\"factor\"] = mu_y.flatten().cpu().numpy()\n",
    "df = df[[\"factor\", \"ret\"]]\n",
    "\n",
    "\n",
    "bt = Backtester(\"factor\", top_pct=0.25).feed(df).run()\n",
    "bt.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl_test))[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7231, -3.5172, -5.7296,  ..., -4.0158, -4.2369, -6.0627],\n",
       "        [-4.8567, -5.5687, -5.0875,  ..., -2.9673, -2.2104, -2.6193],\n",
       "        [-2.1070, -3.9919, -3.9691,  ..., -3.1521, -2.1856, -2.1630],\n",
       "        ...,\n",
       "        [-3.4079, -2.8614, -1.2999,  ..., -1.2931, -2.8400, -3.0919],\n",
       "        [-2.3917, -2.2794, -2.8627,  ..., -0.2281,  0.0670,  0.1173],\n",
       "        [-1.9779, -2.1636, -2.5198,  ..., -4.0418, -4.3209, -4.3645]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0166,  0.0130,  0.0089,  ...,  0.0535,  0.0023,  0.0325],\n",
       "        [ 0.0282,  0.0135,  0.0059,  ...,  0.0553,  0.0020, -0.0018],\n",
       "        [ 0.0024,  0.0078,  0.0039,  ...,  0.0524,  0.0032,  0.0516],\n",
       "        ...,\n",
       "        [ 0.0135,  0.0018,  0.0035,  ..., -0.0238, -0.0084, -0.0438],\n",
       "        [-0.0242, -0.0162, -0.0153,  ..., -0.0516, -0.0017,  0.0105],\n",
       "        [-0.0054,  0.0105,  0.0088,  ...,  0.0860,  0.0080, -0.0128]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_y, Sigma_y, mu_post, sigma_post, mu_prior, sigma_prior = fv(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6542, 0.6387, 0.6164,  ..., 0.7566, 0.7495, 0.6693],\n",
       "        [0.6561, 0.6391, 0.6188,  ..., 0.7559, 0.7499, 0.6700],\n",
       "        [0.6568, 0.6392, 0.6197,  ..., 0.7556, 0.7502, 0.6703],\n",
       "        ...,\n",
       "        [0.6601, 0.6400, 0.6238,  ..., 0.7543, 0.7509, 0.6717],\n",
       "        [0.6693, 0.6421, 0.6353,  ..., 0.7510, 0.7531, 0.6754],\n",
       "        [0.6573, 0.6393, 0.6202,  ..., 0.7554, 0.7502, 0.6705]],\n",
       "       device='cuda:0', grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8794e-04, 7.8169e-01, 5.5017e+00,  ..., 3.7269e+00, 4.8551e+00,\n",
       "         2.3025e+00],\n",
       "        [3.9062e-04, 1.6906e+00, 7.4918e+00,  ..., 2.8093e+00, 4.3920e+00,\n",
       "         2.6159e+00],\n",
       "        [4.5897e-04, 1.9255e+00, 7.6841e+00,  ..., 2.5004e+00, 3.5911e+00,\n",
       "         2.7195e+00],\n",
       "        ...,\n",
       "        [5.4078e-03, 1.1382e+00, 4.5003e+00,  ..., 2.2405e+00, 9.0304e-01,\n",
       "         1.6005e+00],\n",
       "        [3.2812e-04, 2.4362e+00, 8.6974e+00,  ..., 2.6522e+00, 5.6792e-01,\n",
       "         2.6929e+00],\n",
       "        [1.3633e-04, 3.3981e+00, 1.0480e+01,  ..., 2.6282e+00, 5.3939e-01,\n",
       "         3.0180e+00]], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3165e+15, 6.7759e+14, 4.1747e+13, 3.9605e+13, 2.1310e+12, 3.8314e+11,\n",
       "        4.5827e+16, 1.5866e+14, 1.0213e+15,        nan,        nan, 8.8226e+04,\n",
       "        5.3262e+10, 3.5303e+05, 5.8271e+03, 4.0465e+05, 4.2473e+18,        inf,\n",
       "               inf,        inf,        inf, 9.2088e+35, 3.6026e+15, 3.6883e+15,\n",
       "        3.5647e+14, 2.2266e+13, 1.4846e+10, 9.4447e+08, 1.0847e+08, 1.3232e+08,\n",
       "        5.3063e+08, 1.2198e+09, 2.2910e+09, 2.9183e+10, 1.2810e+13, 1.2821e+14,\n",
       "        5.5520e+13, 3.7567e+12, 1.7907e+09, 3.2146e+09, 2.9117e+09, 4.0268e+09,\n",
       "        1.4793e+11, 3.4676e+13, 1.5649e+09, 4.8753e+09, 8.1318e+21, 6.5982e+27,\n",
       "        3.2357e+28, 2.1960e+15, 4.4910e+15, 2.1220e+08, 1.1283e+06, 3.5945e+08,\n",
       "        1.2586e+08, 4.1142e+14, 7.5036e+25, 7.9598e+26, 6.1532e+23, 3.2331e+20,\n",
       "        9.1640e+10, 1.7823e+10, 1.9148e+09, 5.2172e+07, 1.1712e+07, 9.3568e+05,\n",
       "        4.5798e+05, 1.3338e+21,        inf,        inf, 1.7879e+07, 3.5231e+25,\n",
       "        9.6489e+11, 1.7431e+08, 2.6647e+07, 1.4638e+07, 2.6372e+07, 1.4738e+07,\n",
       "        3.3531e+07, 1.5703e+08, 7.3001e+08, 2.8335e+10, 5.0528e+11, 1.1099e+12,\n",
       "        2.3365e+12, 3.1201e+12, 1.1329e+13, 3.7417e+13, 2.6811e+27,        inf,\n",
       "               inf,        inf, 2.1692e+03, 1.9702e+04, 1.6708e+05, 2.7094e+05,\n",
       "        1.0602e+07, 4.2175e+07, 8.5829e+08, 4.2612e+09, 1.5271e+10, 2.2295e+10,\n",
       "        3.9675e+09, 1.1620e+07, 9.6893e+05, 3.8276e+06, 9.5536e+07, 2.7160e+08,\n",
       "        1.8031e+07, 6.6429e+06, 1.1068e+06, 9.8480e+03, 1.0534e+02, 8.5507e+01,\n",
       "        7.4750e+01, 4.5363e+01, 4.4658e+01, 4.2339e+01, 3.8636e+01, 4.0215e+01,\n",
       "        5.0994e+01, 9.2667e+01, 1.3555e+02, 1.6234e+02, 1.7120e+02, 2.0030e+02,\n",
       "        2.1369e+02, 2.3119e+02, 2.1206e+02, 2.0923e+02, 7.8882e+02, 3.3080e+03,\n",
       "        4.6843e+04, 4.7853e+04, 1.8662e+04, 5.5836e+02, 2.5998e+02, 3.3648e+02,\n",
       "        5.5709e+02, 5.6983e+02, 5.9131e+02, 6.8390e+02, 7.1139e+02, 6.9557e+02,\n",
       "        6.2587e+02, 6.5335e+02, 1.9786e+09, 2.4019e+08, 3.6802e+11, 2.3553e+21,\n",
       "        2.6842e+22, 1.5484e+12, 6.0441e+12, 2.1797e+13, 5.2936e+12, 4.4682e+12,\n",
       "        1.9457e+05, 1.7462e+02, 3.0211e+02, 4.8369e+02, 7.2976e+02, 2.6563e+06,\n",
       "        3.4659e+23, 9.4183e+32, 1.1437e+31, 1.0906e+18, 5.6528e+25, 7.4124e+26,\n",
       "        3.0867e+07, 7.9317e+08, 2.3360e+10, 6.1625e+16, 8.3665e+10, 1.3713e+32,\n",
       "        3.5102e+17, 6.2851e+04, 7.1661e+05, 8.9766e+08, 9.6591e+12, 1.8665e+12,\n",
       "        2.7694e+10, 1.0934e+07, 3.2843e+20, 2.6035e+03, 1.6166e+18, 3.0773e+16,\n",
       "        9.9599e+25, 8.0436e+23, 3.4522e+22, 2.7195e+11, 1.9463e+05, 3.0271e+10,\n",
       "        6.9326e+11, 6.2083e+10, 1.7909e+11, 3.1018e+07, 7.6053e+05, 1.9322e+10,\n",
       "        4.5320e+13, 6.5176e+14], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gaussian_kld(\n",
    "    mu1: torch.Tensor, mu2: torch.Tensor, sigma1: torch.Tensor, sigma2: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Calculate KL divergence of two multivariate independent Gaussian distributions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu1 : torch.Tensor\n",
    "        Means of the first Gaussian, B*K\n",
    "    mu2 : torch.Tensor\n",
    "        Means of the second Gaussian, B*K\n",
    "    sigma1 : torch.Tensor\n",
    "        Stds of the first Gaussian, B*K\n",
    "    sigma2 : torch.Tensor\n",
    "        Stds of the second Gaussian, B*K\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        KL divergence, B, denoted as `kld`\n",
    "    \"\"\"\n",
    "    kld_n = (\n",
    "        torch.log(sigma2 / sigma1)\n",
    "        + (sigma1**2 + (mu1 - mu2) ** 2) / sigma2**2 / 2\n",
    "        - 0.5\n",
    "    )\n",
    "    return kld_n.sum(-1)\n",
    "\n",
    "gaussian_kld(mu_post, mu_prior, sigma_post, sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0726, 0.0709, 0.0450,  ..., 0.0660, 0.0697, 0.0853],\n",
       "        [0.0878, 0.0915, 0.0825,  ..., 0.0691, 0.0613, 0.0662],\n",
       "        [0.0698, 0.0844, 0.0876,  ..., 0.0442, 0.0652, 0.0691],\n",
       "        ...,\n",
       "        [0.0700, 0.0909, 0.1109,  ..., 0.0870, 0.0551, 0.0552],\n",
       "        [0.0541, 0.0588, 0.0688,  ..., 0.0995, 0.0895, 0.0877],\n",
       "        [0.0554, 0.0553, 0.0552,  ..., 0.1174, 0.1153, 0.1078]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0166,  0.0130,  0.0089,  ...,  0.0535,  0.0023,  0.0325],\n",
       "        [ 0.0282,  0.0135,  0.0059,  ...,  0.0553,  0.0020, -0.0018],\n",
       "        [ 0.0024,  0.0078,  0.0039,  ...,  0.0524,  0.0032,  0.0516],\n",
       "        ...,\n",
       "        [ 0.0135,  0.0018,  0.0035,  ..., -0.0238, -0.0084, -0.0438],\n",
       "        [-0.0242, -0.0162, -0.0153,  ..., -0.0516, -0.0017,  0.0105],\n",
       "        [-0.0054,  0.0105,  0.0088,  ...,  0.0860,  0.0080, -0.0128]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
